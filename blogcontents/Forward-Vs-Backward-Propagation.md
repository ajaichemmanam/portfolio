---
title: "ğ…ğ¨ğ«ğ°ğšğ«ğ ğ•ğ¬ ğğšğœğ¤ğ°ğšğ«ğ ğğ«ğ¨ğ©ğšğ ğšğ­ğ¢ğ¨ğ§"
author: "Ajai Chemmanam"
date: "2022-04-03"
---

## ğ…ğ¨ğ«ğ°ğšğ«ğ ğ•ğ¬ ğğšğœğ¤ğ°ğšğ«ğ ğğ«ğ¨ğ©ğšğ ğšğ­ğ¢ğ¨ğ§

### ğ—™ğ—¼ğ—¿ğ˜„ğ—®ğ—¿ğ—± ğ—£ğ—¿ğ—¼ğ—½ğ—®ğ—´ğ—®ğ˜ğ—¶ğ—¼ğ—»

- The process of going from Input Layer to Output Layer to adjust or correct the weights is called Forward Propagation.

- It calculates the output vector from the input vector from which the loss can be calculated. The weights in the hidden layer are not adjusted in this case.

### ğğšğœğ¤ğ°ğšğ«ğ ğğ«ğ¨ğ©ğšğ ğšğ­ğ¢ğ¨ğ§

- The process of going backwards i.e, from the Output Layer to the Input Layer is called Backward Propagation.

- This method is preferred for adjusting the weights of hidden layers to reach minimum loss.
