---
title: "Learning Rate Hyperparameter in Neural Networks"
author: "Ajai Chemmanam"
date: "2022-04-09"
---

## Learning Rate

Learning rate is a hyperparameter which controls the rate at which the weights of a network are updated.

It controls how quickly a model adapts to the problem, small learning rate means more training due to smaller changes to weights & larger rates mean rapid changes in weight values.

The values or learning rates generally lie in the range (0,1).

#### ğ—£ğ—¿ğ—¼ğ—¯ğ—¹ğ—²ğ—ºğ˜€:

- If the values of learning rates are too high the model converges to a suboptimal solution

- If the learning rate is too small the weights do not get updated and training gets stuck.

#### ğ—§ğ—µğ—²ğ˜€ğ—² ğ—½ğ—¿ğ—¼ğ—¯ğ—¹ğ—²ğ—ºğ˜€ ğ—°ğ—®ğ—» ğ—¯ğ—² ğ˜ğ—®ğ—°ğ—¸ğ—¹ğ—²ğ—± ğ—¯ğ˜†:

- Decreasing the value of the learning rate with each increasing epoch.

- Dropping the value of the learning rate with a predetermined value at regular intervals.

- Increasing or decreasing the learning rate based on gradient values.
